{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s14YH6D4pBzq"
   },
   "source": [
    "# Buscalibre Numeric Model\n",
    "\n",
    "The preprocessed has two types of data, numeric and text data. First, we build an estimator using only the numeric part, based on different Machine Learning Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L29d5QCts0CS"
   },
   "source": [
    "## Libraries and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX6IKSn4s7T9"
   },
   "source": [
    "Import the neccesary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuxUuKYqszh3",
    "outputId": "1e8cdc5d-bae4-4018-bc01-b15f0fafa01e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xJSKoY5tHRS"
   },
   "source": [
    "Upload the Training dataset, and shuffle as it becomes sectioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = os.getcwd().replace(\"\\\\\", \"/\") + \"/\"\n",
    "path_parent = os.path.dirname(os.getcwd()).replace(\"\\\\\", \"/\") + \"/\"\n",
    "train = pd.read_csv(path_parent + \"data_analysis/train_2.csv\")\n",
    "test = pd.read_csv(path_parent + \"data_analysis/test_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "95Cbr5NztI4I"
   },
   "outputs": [],
   "source": [
    "train = train.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "X = train.drop(columns=[\"isbn\", \"review\", \"topic\", \"review_cleaned\"])\n",
    "y = train[\"topic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GH1PZkCwtfid"
   },
   "source": [
    "Define the cross validated scorer function.\n",
    "\n",
    "Note: As we saw in the Exploratory Analysis part, the target labels are imbalanced, so we are the following metrics:\n",
    "\n",
    "- [wieghted f1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html): instead of accuracy, suitable for imbalanced multiclass target data.\n",
    "- [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix): to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oEThnwRGtp5Z"
   },
   "outputs": [],
   "source": [
    "def cross_score(model, k=10):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring=\"f1_weighted\")\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, one would validate the scores of a model using the cross validation method, but we are splitting the data to analyze some of the model behaviours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holding set shapes: (2130, 26), (2130,)\n",
      "Validation set shapes: (533, 26), (533,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_hold, X_val, y_hold, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=999)\n",
    "print(f\"Holding set shapes: {X_hold.shape}, {y_hold.shape}\")\n",
    "print(f\"Validation set shapes: {X_val.shape}, {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46BiZN245Ws8",
    "tags": []
   },
   "source": [
    "### Common Label\n",
    "\n",
    "The topic (target label) \"grandes-descuentos\" has the greatest number of samples. So an starting prediction is to assume that every label belongs to it.\n",
    "\n",
    "If we do so, we get an score of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CoQHDs_5WUj",
    "outputId": "83a35838-4a3d-40c7-cc63-1f14e2d4cf7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with the common label has an f1 score of: 0.24682589316735654.\n",
      "\n",
      "And the following Validation confusion matrix: \n",
      " [[  0   0   0   0   0   0   0   0   0  23   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  38   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  17   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  88   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  46   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  15   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   8   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   3   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 223   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  31   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  18   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   9   0   0   0]] \n",
      "\n",
      "Wall time: 143 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = [\"grandes-descuentos\" for i in range(len(y_val))]\n",
    "score = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "print(f\"Predicting with the common label has an f1 score of: {score}.\\n\")\n",
    "print(f\"And the following Validation confusion matrix: \\n {confusion_matrix(y_val, y_pred)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vslyx9Imt29Y",
    "tags": []
   },
   "source": [
    "## Base Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkYo8frHuD9-"
   },
   "source": [
    "We are going to use multiple simple predictors provided by [Scikit-Learn](https://scikit-learn.org/) and [Tensorflow](https://www.tensorflow.org). \n",
    "\n",
    "The choice of hyperparameters was done with the help of [Optuna package](https://optuna.org/) using [Google Colab](https://colab.research.google.com/) servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENQ0vPftvbFb"
   },
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKRHgf7avZGq",
    "outputId": "ffff656c-bf37-4a96-c1b4-983296abaa3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier has a cross validated f1 score of: 0.6582078030807285. \n",
      "\n",
      "And the following Validation confusion matrix: \n",
      " [[ 15   5   0   0   0   0   0   0   0   3   0   0   0]\n",
      " [  7  21   0   4   0   0   0   0   0   4   1   1   0]\n",
      " [  0   0   5   4   5   0   0   1   0   2   0   0   0]\n",
      " [  0   2   1  67   3   0   0   0   0  10   4   1   0]\n",
      " [  1   1   2   9  21   2   0   2   0   7   0   1   0]\n",
      " [  0   0   0   8   3   3   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   6   1   0   0   0   1   0]\n",
      " [  0   1   0   1   3   0   0   4   0   2   2   1   0]\n",
      " [  0   0   0   1   2   0   0   0   0   0   0   0   0]\n",
      " [  0   5   0   3   2   0   0   1   0 209   0   3   0]\n",
      " [  0   3   0  10   0   0   0   0   0   4  14   0   0]\n",
      " [  0   4   0   3   1   0   0   0   0   4   0   6   0]\n",
      " [  0   0   0   7   0   0   0   0   0   0   2   0   0]] \n",
      "\n",
      "Wall time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ovr_rfc = OneVsRestClassifier(RandomForestClassifier(**{\n",
    "    'n_estimators': 334,\n",
    "    'criterion': 'entropy',\n",
    "    'max_depth': 42,\n",
    "    'min_samples_split': 9,\n",
    "    'min_samples_leaf': 4,\n",
    "    'max_features': 0.45448755763486154,\n",
    "    'random_state': 555\n",
    "}))\n",
    "score = cross_score(ovr_rfc)\n",
    "print(f\"Random Forest Classifier has a cross validated f1 score of: {score}. \\n\")\n",
    "ovr_rfc.fit(X_hold, y_hold)\n",
    "ovr_rfc_pred = ovr_rfc.predict(X_val)\n",
    "print(f\"And the following Validation confusion matrix: \\n {confusion_matrix(y_val, ovr_rfc_pred)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHPCOkjqvw1K"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9LUxR7ivvAN",
    "outputId": "341cfa8a-8a96-4368-eeb2-abb6579a7e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression has a cross validated f1 score of: 0.642663188564323. \n",
      "\n",
      "And the following Validation confusion matrix: \n",
      " [[ 15   3   0   0   1   0   0   0   0   4   0   0   0]\n",
      " [  4  19   0   5   0   0   0   0   0   7   3   0   0]\n",
      " [  0   0   5   5   3   0   1   2   0   1   0   0   0]\n",
      " [  0   3   0  68   2   0   0   0   0  10   4   1   0]\n",
      " [  1   2   4  12  16   1   0   2   0   8   0   0   0]\n",
      " [  0   1   0   7   3   3   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   6   2   0   0   0   0   0]\n",
      " [  0   1   0   6   1   0   0   3   0   1   0   2   0]\n",
      " [  0   0   0   1   2   0   0   0   0   0   0   0   0]\n",
      " [  1   3   0   6   3   0   0   0   0 209   0   1   0]\n",
      " [  1   2   0   9   0   0   0   0   0   6  13   0   0]\n",
      " [  0   1   0   4   1   0   0   0   0   5   0   7   0]\n",
      " [  0   0   0   7   0   0   0   0   0   0   2   0   0]] \n",
      "\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logreg = make_pipeline(StandardScaler(), MinMaxScaler(), LogisticRegression(\n",
    "    C=8.261486231908338,\n",
    "    tol=0.8728213920467933,\n",
    "    intercept_scaling=9.117615728181427,\n",
    "    multi_class=\"multinomial\",\n",
    "    max_iter=10_000,\n",
    "    random_state=555\n",
    "))\n",
    "score = cross_score(logreg)\n",
    "print(f\"Logistic Regression has a cross validated f1 score of: {score}. \\n\")\n",
    "logreg.fit(X_hold, y_hold)\n",
    "logreg_pred = logreg.predict(X_val)\n",
    "print(f\"And the following Validation confusion matrix: \\n {confusion_matrix(y_val, logreg_pred)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wtk4Ywbmv1zt"
   },
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwtaRxAev0X9",
    "outputId": "0be474f8-96c0-4a53-c708-9a25694b7385",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier has a cross validated f1 score of: 0.6586022709564061. \n",
      "\n",
      "And the following Validation confusion matrix: \n",
      " [[ 14   4   0   0   0   0   0   0   0   5   0   0   0]\n",
      " [  7  22   0   2   0   0   0   0   0   4   3   0   0]\n",
      " [  0   0   5   2   6   0   1   1   0   2   0   0   0]\n",
      " [  0   3   0  64   3   1   0   1   0   7   6   1   2]\n",
      " [  1   1   1  11  22   1   0   1   1   6   0   1   0]\n",
      " [  0   0   0   8   2   2   0   0   1   0   1   1   0]\n",
      " [  0   0   0   0   0   0   5   2   0   0   0   1   0]\n",
      " [  1   1   0   3   2   0   0   5   0   1   0   1   0]\n",
      " [  0   0   1   1   1   0   0   0   0   0   0   0   0]\n",
      " [  1   6   0   3   2   1   0   0   0 208   0   2   0]\n",
      " [  0   3   0   9   0   0   0   1   0   5  13   0   0]\n",
      " [  0   3   0   4   1   0   0   0   0   3   0   7   0]\n",
      " [  0   0   0   6   0   0   0   0   0   1   1   0   1]] \n",
      "\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ovr_xgb = OneVsRestClassifier(XGBClassifier(**{\n",
    "    'n_estimators': 378,\n",
    "    'learning_rate': 0.02950073992817461,\n",
    "    'base_score': 0.9187179242725662,\n",
    "    'verbosity': 0,\n",
    "    'use_label_encoder': False,\n",
    "    'random_state': 555\n",
    "}))\n",
    "score = cross_score(ovr_xgb)\n",
    "print(f\"XGBoost Classifier has a cross validated f1 score of: {score}. \\n\")\n",
    "ovr_xgb.fit(X_hold, y_hold)\n",
    "ovr_xgb_pred = ovr_xgb.predict(X_val)\n",
    "print(f\"And the following Validation confusion matrix: \\n {confusion_matrix(y_val, ovr_xgb_pred)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDnh4gb0v7Ze"
   },
   "source": [
    "### Light GBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vW1XlQITv-GJ",
    "outputId": "b6fdd221-e7bd-4c2f-c50d-1715bf3d0042",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM Classifier has a cross validated f1 score of: 0.6452160708723848. \n",
      "\n",
      "And the following Validation confusion matrix: \n",
      " [[ 15   5   0   0   1   0   0   0   0   2   0   0   0]\n",
      " [  7  20   0   3   0   0   0   0   0   6   2   0   0]\n",
      " [  0   0   2   2   6   0   1   2   1   3   0   0   0]\n",
      " [  0   3   0  64   6   0   0   0   0   8   5   1   1]\n",
      " [  1   0   4  10  19   3   0   1   0   7   0   1   0]\n",
      " [  0   0   0   6   4   2   0   0   0   1   1   0   1]\n",
      " [  0   0   0   0   0   0   5   2   0   0   0   1   0]\n",
      " [  1   1   0   3   1   0   0   4   0   2   1   1   0]\n",
      " [  0   0   0   1   2   0   0   0   0   0   0   0   0]\n",
      " [  2   5   0   2   3   0   0   1   0 208   0   2   0]\n",
      " [  1   2   0   6   1   0   0   1   0   5  14   1   0]\n",
      " [  0   3   0   4   1   0   0   1   0   3   0   6   0]\n",
      " [  0   0   0   6   0   0   0   0   0   1   1   0   1]] \n",
      "\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgb = LGBMClassifier(**{\n",
    "    'num_leaves': 27,\n",
    "    'n_estimators': 268,\n",
    "    'learning_rate': 0.018813923117324143,\n",
    "    'random_state': 555\n",
    "})\n",
    "score = cross_score(lgb)\n",
    "print(f\"LGBM Classifier has a cross validated f1 score of: {score}. \\n\")\n",
    "lgb.fit(X_hold, y_hold)\n",
    "lgb_pred = lgb.predict(X_val)\n",
    "print(f\"And the following Validation confusion matrix: \\n {confusion_matrix(y_val, lgb_pred)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRCWb0PUwCwX"
   },
   "source": [
    "### CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLOXGbTDwEFC",
    "outputId": "99a538bb-618a-4f16-ba67-18abd5fb7f22",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classifier has a cross validated f1 score of: 0.6505277205855613. \n",
      "\n",
      "And the following Validation confusion matrix: \n",
      " [[ 15   3   0   0   0   0   0   0   0   5   0   0   0]\n",
      " [  6  20   0   4   0   0   0   0   0   4   4   0   0]\n",
      " [  0   0   6   3   3   0   0   2   1   2   0   0   0]\n",
      " [  0   2   1  64   3   0   0   0   0  10   6   1   1]\n",
      " [  1   2   4   7  21   2   0   2   0   5   0   2   0]\n",
      " [  0   0   0   7   4   3   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   6   1   0   0   0   1   0]\n",
      " [  0   1   0   1   3   0   0   4   0   2   2   1   0]\n",
      " [  0   0   0   1   2   0   0   0   0   0   0   0   0]\n",
      " [  1   5   0   4   1   0   0   0   0 209   0   3   0]\n",
      " [  0   5   0   7   0   0   0   1   0   3  14   0   1]\n",
      " [  0   4   0   3   1   0   0   0   0   3   0   7   0]\n",
      " [  0   0   0   6   0   0   0   0   0   0   2   0   1]] \n",
      "\n",
      "Wall time: 7min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ovr_cat = OneVsRestClassifier(CatBoostClassifier(**{\n",
    "    'iterations': 330,\n",
    "    'learning_rate': 0.043379595491767745,\n",
    "    'depth': 7,\n",
    "    'l2_leaf_reg': 0.5416613355579589,\n",
    "    'border_count': 212,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'verbose': False,\n",
    "    'random_state': 555\n",
    "}))\n",
    "score = cross_score(ovr_cat)\n",
    "print(f\"CatBoost Classifier has a cross validated f1 score of: {score}. \\n\")\n",
    "ovr_cat.fit(X_hold, y_hold)\n",
    "ovr_cat_pred = ovr_cat.predict(X_val)\n",
    "print(f\"And the following Validation confusion matrix: \\n {confusion_matrix(y_val, ovr_cat_pred)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa71a5bRwGI2"
   },
   "source": [
    "### Forward Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZThaZpKwJ9b",
    "outputId": "11532db3-34e7-4ca2-be77-816a43e7c085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Classifier has a cross validated f1 score of: 0.6458845412913149. \n",
      "\n",
      "And the following Validation confusion matrix: \n",
      " [[ 16   3   0   0   0   0   0   0   0   4   0   0   0]\n",
      " [  7  22   0   4   0   0   0   2   0   2   1   0   0]\n",
      " [  0   0   3   3   6   0   0   3   0   1   0   1   0]\n",
      " [  0   6   0  66   2   0   0   6   0   5   2   1   0]\n",
      " [  1   1   1  10  21   0   1   7   0   4   0   0   0]\n",
      " [  0   1   0   8   3   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1   5   2   0   0   0   0   0]\n",
      " [  0   1   0   2   3   0   0   5   0   1   0   2   0]\n",
      " [  0   0   0   1   2   0   0   0   0   0   0   0   0]\n",
      " [  5   4   0   4   5   1   0   4   0 194   4   2   0]\n",
      " [  1   4   0   5   0   0   0   3   0   4  13   1   0]\n",
      " [  0   3   0   3   2   0   0   0   0   3   0   7   0]\n",
      " [  0   0   0   7   0   0   0   0   0   0   2   0   0]] \n",
      "\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(555)\n",
    "class NetworkClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, ini_neurons=60, optimizer=\"adam\", epochs=200):\n",
    "        self.ini_neurons = ini_neurons\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.model = Sequential()\n",
    "    # Fit Function, fit the Neural Network with one layer and ini_neurons number of neurons.\n",
    "    def fit(self, X, y):\n",
    "        y_enc = pd.get_dummies(y)\n",
    "        self.cols = y_enc.columns\n",
    "        y_np = y_enc.to_numpy()\n",
    "        X_np = X.to_numpy()\n",
    "        self.model.add(Dense(self.ini_neurons, input_shape=(X.shape[1], ), activation=\"relu\"))\n",
    "        self.model.add(Dense(13, activation=\"softmax\"))\n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "        self.model.fit(\n",
    "            X_np, y_np, epochs=self.epochs, verbose=0\n",
    "        )\n",
    "        return self\n",
    "    # Predict Function\n",
    "    def predict(self, X):\n",
    "        X_np = X.to_numpy()\n",
    "        y_hat = self.model.predict(X_np)\n",
    "        y_df = pd.DataFrame(data=y_hat, columns=self.cols)\n",
    "        y_pred = y_df.idxmax(axis=1)\n",
    "        return y_pred\n",
    "\n",
    "nc = NetworkClassifier()\n",
    "score = cross_score(nc)\n",
    "print(f\"Network Classifier has a cross validated f1 score of: {score}. \\n\")\n",
    "nc.fit(X_hold, y_hold)\n",
    "nc_pred = nc.predict(X_val)\n",
    "print(f\"And the following Validation confusion matrix: \\n {confusion_matrix(y_val, nc_pred)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost every model fits very well to the training data with an acceptable f1 score. To know which model we would use, first verify the correlation in the validation predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "stack = np.column_stack([\n",
    "    le.transform(ovr_rfc_pred), \n",
    "    le.transform(logreg_pred), \n",
    "    le.transform(ovr_xgb_pred), \n",
    "    le.transform(lgb_pred), \n",
    "    le.transform(ovr_cat_pred), \n",
    "    le.transform(nc_pred)\n",
    "])\n",
    "stack = pd.DataFrame(data=stack, columns=[\"rfc\", \"logreg\", \"xgb\", \"lgb\", \"catb\", \"network\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfc</th>\n",
       "      <th>logreg</th>\n",
       "      <th>xgb</th>\n",
       "      <th>lgb</th>\n",
       "      <th>catb</th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rfc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.825543</td>\n",
       "      <td>0.856202</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.893799</td>\n",
       "      <td>0.817743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg</th>\n",
       "      <td>0.825543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.743020</td>\n",
       "      <td>0.724786</td>\n",
       "      <td>0.810144</td>\n",
       "      <td>0.833218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>0.856202</td>\n",
       "      <td>0.743020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.868415</td>\n",
       "      <td>0.825789</td>\n",
       "      <td>0.753540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgb</th>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.724786</td>\n",
       "      <td>0.868415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827099</td>\n",
       "      <td>0.730183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catb</th>\n",
       "      <td>0.893799</td>\n",
       "      <td>0.810144</td>\n",
       "      <td>0.825789</td>\n",
       "      <td>0.827099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>network</th>\n",
       "      <td>0.817743</td>\n",
       "      <td>0.833218</td>\n",
       "      <td>0.753540</td>\n",
       "      <td>0.730183</td>\n",
       "      <td>0.772371</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rfc    logreg       xgb       lgb      catb   network\n",
       "rfc      1.000000  0.825543  0.856202  0.846000  0.893799  0.817743\n",
       "logreg   0.825543  1.000000  0.743020  0.724786  0.810144  0.833218\n",
       "xgb      0.856202  0.743020  1.000000  0.868415  0.825789  0.753540\n",
       "lgb      0.846000  0.724786  0.868415  1.000000  0.827099  0.730183\n",
       "catb     0.893799  0.810144  0.825789  0.827099  1.000000  0.772371\n",
       "network  0.817743  0.833218  0.753540  0.730183  0.772371  1.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have two suggested models: The Random Forest Classifier which got the best cross validated f1 score, and a combination of the least linear-correlated models, Logistic Regression + LightGBM + Network Classifier. We are going to fit the combination in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGIqkz8DwSEP"
   },
   "source": [
    "## Weighted Voting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0Wo_sggxVMz"
   },
   "source": [
    "One simple way to combine and (posible) improve your predictions, is building a [Voting Classifier](https://en.wikipedia.org/wiki/Ensemble_learning). Each Estimator makes its own prediction, and then we save the most voted label. Also, one can put weights on each estimator, to favor it over the others, and get an overall improved prediction.\n",
    "\n",
    "At this time, weights were chosen using Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBaovUkqzX-H"
   },
   "source": [
    "Define and call the Weighted Averaging Estimator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "s_XYJzBvwQ5-"
   },
   "outputs": [],
   "source": [
    "class WeightedAveragingEstimator(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models, weights=None):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "    # Fit Function, fit cloned models to prevent overwriting in cross validation.\n",
    "    def fit(self, X, y):\n",
    "        self.cols = pd.get_dummies(y).columns\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "    # Predict Function, make the prediction for each model and maintain the most voted.\n",
    "    def predict(self, X):\n",
    "        sum_ = pd.DataFrame(dtype=float, columns=self.cols)\n",
    "        for i, model in enumerate(self.models_):\n",
    "            y_pred_ = model.predict(X)\n",
    "            y_hat = self.weights[i] * pd.get_dummies(y_pred_)\n",
    "            sum_ = sum_.add(y_hat, fill_value=0)\n",
    "        sum_.fillna(value=0)\n",
    "        y_pred = sum_.idxmax(axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = (\n",
    "    make_pipeline(StandardScaler(), MinMaxScaler(), LogisticRegression(\n",
    "        C=8.261486231908338,\n",
    "        tol=0.8728213920467933,\n",
    "        intercept_scaling=9.117615728181427,\n",
    "        multi_class=\"multinomial\",\n",
    "        max_iter=10_000,\n",
    "        random_state=555\n",
    "    )),\n",
    "    LGBMClassifier(**{\n",
    "        'num_leaves': 27,\n",
    "        'n_estimators': 268,\n",
    "        'learning_rate': 0.018813923117324143,\n",
    "        'random_state': 555\n",
    "    }),\n",
    "    NetworkClassifier()\n",
    ")\n",
    "weights = [0.3, 0.4, 0.3]\n",
    "wae = WeightedAveragingEstimator(models=models, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXMpoYBX0CwU",
    "outputId": "ff51adf0-a8ba-406d-efbf-47613814880a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Averaging Estimator has a cross validated f1 score of: 0.6496798494514795. \n",
      "\n",
      "And the following Validation confusion matrix: \n",
      " [[ 15   4   0   0   0   0   0   0   0   4   0   0   0]\n",
      " [  7  19   0   6   0   0   0   0   0   4   2   0   0]\n",
      " [  0   0   3   4   5   0   1   2   1   1   0   0   0]\n",
      " [  0   4   0  69   3   0   0   0   0   7   4   1   0]\n",
      " [  1   2   3  11  19   2   0   2   0   6   0   0   0]\n",
      " [  0   1   0   7   4   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   6   2   0   0   0   0   0]\n",
      " [  0   1   0   5   1   0   0   4   0   1   0   2   0]\n",
      " [  0   0   0   1   2   0   0   0   0   0   0   0   0]\n",
      " [  1   5   0   4   3   0   0   1   0 206   0   3   0]\n",
      " [  1   3   0   8   0   0   0   1   0   5  12   1   0]\n",
      " [  0   3   0   4   2   0   0   0   0   3   0   6   0]\n",
      " [  0   0   0   7   0   0   0   0   0   0   2   0   0]] \n",
      "\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score = cross_score(wae)\n",
    "print(f\"Weighted Averaging Estimator has a cross validated f1 score of: {score}. \\n\")\n",
    "wae.fit(X_hold, y_hold)\n",
    "y_pred = wae.predict(X_val)\n",
    "print(f\"And the following Validation confusion matrix: \\n {confusion_matrix(y_val, y_pred)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models worked fine. But we save the CatBoost Classifier model as it got the best rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build a classifier that only takes the review data as inputs, and then compare it with the numeric model (the CatBoostClassifier) and see if we can get improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = os.getcwd().replace(\"\\\\\", \"/\") + \"/\"\n",
    "path_parent = os.path.dirname(os.getcwd()).replace(\"\\\\\", \"/\") + \"/\"\n",
    "train = pd.read_csv(path_parent + \"data_analysis/train_2.csv\")\n",
    "test = pd.read_csv(path_parent + \"data_analysis/test_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "X = train[\"review_cleaned\"]\n",
    "y = train[\"topic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machines only understand numbers, so to process text with a classifier, we have to represent each word as a vector in some vector space.\n",
    "\n",
    "There are many ways to do that. One of the easiest is to use a pre-trained word embedding matrix. Here we are using the [GloVe Embedding](https://nlp.stanford.edu/projects/glove/) to map each word into a 300-dimension real vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two conjugate words can be quite different but not alter the context of a sentence. It is always a good idea to standardize each word and stay wit its baseline meaning to make it easier for the classifier. That process is known as lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the spanish lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_spanish_lemmatizer.main.SpacyCustomLemmatizer at 0x2dcaf96cc40>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import spacy_spanish_lemmatizer\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.replace_pipe(\"lemmatizer\", \"spanish_lemmatizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the lemmatizer function and apply it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def lemmatizer(X):\n",
    "    start = time.time()\n",
    "    lemma = X.apply(lambda x: \" \".join([token.lemma_ for token in nlp(x)]))\n",
    "    end = time.time()\n",
    "    print(f\"Lemmatization Done in {(end - start)//60:.2f} minutes\")\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Done in 14.00 minutes\n"
     ]
    }
   ],
   "source": [
    "X_lemma = lemmatizer(X)\n",
    "X_lemma = pd.DataFrame(data=X_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with each word and its corresponding vector in the GloVe Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000654it [01:43, 9701.24it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "embedding_vector = {}\n",
    "with open(path_folder + \"SBW-vectors-300-min5.txt\", encoding=\"utf8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        value = line.split(\" \")\n",
    "        word = value[0]\n",
    "        coef = np.array(value[1:], dtype=\"float32\")\n",
    "        embedding_vector[word] = coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a datafrane of shape (*, 300) containing each vector, and each word as indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_df(X):\n",
    "    X_vocab = [item for i in range(X_lemma.shape[0]) for item in X_lemma.iloc[i][0].split(\" \")]\n",
    "    X_vocab = list(set(X_vocab))\n",
    "    vocab_df = pd.DataFrame(data=[], columns=[str(i) for i in range(300)])\n",
    "    for word in tqdm(X_vocab):\n",
    "        try:\n",
    "            temp = embedding_vector[word]\n",
    "            temp = pd.Series(temp, name=word)\n",
    "            vocab_df.at[word, :] = temp.values\n",
    "        except:\n",
    "            pass\n",
    "    return vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 28355/28355 [26:49<00:00, 17.62it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_df = get_vocab_df(X_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have each word embedded in a vector space, we must cluster them in different sets of similar words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the [Gaussian Mixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 100\n",
    "clustering = GaussianMixture(n_components=n_clusters)\n",
    "clustering.fit(vocab_df)\n",
    "labels = clustering.predict(vocab_df)\n",
    "clusters = np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the cluster labels in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_labels = pd.Series(data=labels, name=\"label\", index=vocab_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataset in which the columns are the cluster labels and the values is the number of words belonging to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2663/2663 [00:09<00:00, 285.12it/s]\n"
     ]
    }
   ],
   "source": [
    "zeros_mat = np.zeros(shape=(X.shape[0], n_clusters))\n",
    "X_new = pd.DataFrame(data=zeros_mat, columns=[str(i) for i in range(n_clusters)])\n",
    "for i in tqdm(range(X.shape[0])):\n",
    "    for word in X_lemma.iloc[i][0].split(\" \"):\n",
    "        if word in vocab_df.index:\n",
    "            col = str(words_labels[word])\n",
    "            X_new.at[i, col] = X_new.at[i, col] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier that translates the number of clusters that each row has, should be as simple as possible. We will use the Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix: [[ 43   8   0   0   0   0   0   0   0  61   1   0   0]\n",
      " [  5  65   0   2   0   0   0   2   0 109   4   2   0]\n",
      " [  0   0  25   3   0   1   0   0   1  53   1   0   0]\n",
      " [  3   6   0 153   3   3   0   2   0 253  13   1   1]\n",
      " [  0   3   1  19  40   0   0   6   0 157   0   1   2]\n",
      " [  0   0   2  14   2  16   0   1   0  41   0   0   0]\n",
      " [  0   0   0   0   0   0  39   0   0   0   0   0   0]\n",
      " [  0   1   0   3   6   0   0  27   0  30   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0  14   4   0   0   0]\n",
      " [ 12  11   9  56  19   2   2   7   1 980   8   5   1]\n",
      " [  3  17   0  23   1   0   0   1   0  62  46   2   2]\n",
      " [  3   4   0   2   0   0   0   0   0  60   0  24   0]\n",
      " [  0   0   0   5   0   1   0   0   0  17   1   0  21]] \n",
      "\n",
      "Training f1 Score:\n",
      "0.522565289069367\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(LogisticRegression(\n",
    "    max_iter=10_000,\n",
    "    random_state=555\n",
    "))\n",
    "clf.fit(X_new, y)\n",
    "y_pred = clf.predict(X_new)\n",
    "print(f\"Training Confusion Matrix: {confusion_matrix(y, y_pred)} \\n\")\n",
    "print(f\"Training f1 Score:\")\n",
    "print(f1_score(y, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can perform a more elaborated scraping to gather (somehow) an extract of each book instead of a review inserted by someone in the website. There are some totally corrupted reviews, and other books that just don't have a review. We will stick to the Numeric CatBoost Classifier with a final scores of:"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Buscalibre_Numeric_ML_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
